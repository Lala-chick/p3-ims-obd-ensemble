{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trash_seg_inference.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1YwzIYlHHoRd4EeY_obAVucM8mbNSvbnH","authorship_tag":"ABX9TyOHptMFhtsC1qBzWjrUGef1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f6VQiBGMl2_","executionInfo":{"status":"ok","timestamp":1619868045618,"user_tz":-540,"elapsed":180555,"user":{"displayName":"주영이","photoUrl":"","userId":"06973549239085587415"}},"outputId":"63fcd062-bcc0-406a-cbed-7586556f9b3d"},"source":["!pip install segmentation_models_pytorch\n","!pip install -q -U albumentations\n","!pip install timm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.9.1+cu101)\n","Requirement already satisfied: timm==0.3.2 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.3.2)\n","Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.6.3)\n","Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.7.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (7.1.2)\n","Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.8.1+cu101)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.41.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision>=0.3.0->segmentation_models_pytorch) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n","Collecting git+https://github.com/zhanghang1989/PyTorch-Encoding.git\n","  Cloning https://github.com/zhanghang1989/PyTorch-Encoding.git to /tmp/pip-req-build-slhdewra\n","  Running command git clone -q https://github.com/zhanghang1989/PyTorch-Encoding.git /tmp/pip-req-build-slhdewra\n","Requirement already satisfied (use --upgrade to upgrade): torch-encoding==1.2.2b20210501 from git+https://github.com/zhanghang1989/PyTorch-Encoding.git in /usr/local/lib/python3.7/dist-packages\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (4.41.1)\n","Requirement already satisfied: nose in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (1.3.7)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (2.3.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (1.8.1+cu101)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (0.9.1+cu101)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (7.1.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-encoding==1.2.2b20210501) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->torch-encoding==1.2.2b20210501) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-encoding==1.2.2b20210501) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-encoding==1.2.2b20210501) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-encoding==1.2.2b20210501) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-encoding==1.2.2b20210501) (2.10)\n","Building wheels for collected packages: torch-encoding\n","  Building wheel for torch-encoding (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n","Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.3.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->timm) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kbvfvQZxM5vt"},"source":["import segmentation_models_pytorch as smp\n","from tqdm import tqdm\n","import gc\n","\n","import math\n","from torch.optim.optimizer import Optimizer, required\n","\n","\n","from sklearn.model_selection import GroupKFold, KFold\n","import torch\n","from torch import nn\n","import torchvision\n","import cv2\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts, _LRScheduler\n","from scipy.ndimage.interpolation import zoom\n","import albumentations as A\n","from torch.nn import functional as F\n","from albumentations.pytorch import ToTensorV2\n","\n","from pycocotools.coco import COCO\n","\n","import matplotlib.pyplot as plt\n","import sys\n","import time\n","import random\n","import timm\n","\n","import zipfile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BryIBooEts5R"},"source":["CFG = {\n","    \"encoder\": \"timm-efficientnet-b4\"\n","    \"mean\": (0.485, 0.456, 0.406),\n","    \"std\": (0.229, 0.224, 0.225)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nK9fAPM8PNhF","executionInfo":{"status":"ok","timestamp":1619877823188,"user_tz":-540,"elapsed":543,"user":{"displayName":"주영이","photoUrl":"","userId":"06973549239085587415"}}},"source":["device = \"cuda\"\n","submission = pd.read_csv('/content/drive/MyDrive/trash_segmentation/data/sample_submission.csv', index_col=None)\n","test_path = '/content/drive/MyDrive/trash_segmentation/data/test.json'\n","MODEL_PATHS = ['/content/drive/MyDrive/trash_segmentation/models/timm-efficientnet-b4_0.pth']\n","SUBMISSION_PATH = \"/content/drive/MyDrive/trash_segmentation/data/submission.csv\"\n","OUT_MASKS = f'/content/drive/MyDrive/trash_segmentation/masks.zip'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7t054mcM5x5"},"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKpBk1eJM50L"},"source":["class CustomDataLoader(Dataset):\n","    \"\"\"COCO format\"\"\"\n","    def __init__(self, data_dir, mode = 'train', transform = None):\n","        super().__init__()\n","        self.mode = mode\n","        self.transform = transform\n","        self.coco = COCO(data_dir)\n","        \n","    def __getitem__(self, index: int):\n","        # dataset이 index되어 list처럼 동작\n","        image_id = self.coco.getImgIds(imgIds=index)\n","        image_infos = self.coco.loadImgs(image_id)[0]\n","        \n","        # cv2 를 활용하여 image 불러오기\n","        images = cv2.imread(os.path.join('/content/drive/MyDrive/trash_segmentation/data', image_infos['file_name']))\n","        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n","        \n","        if (self.mode in ('train', 'val')):\n","            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n","            anns = self.coco.loadAnns(ann_ids)\n","\n","            # Load the categories in a variable\n","            cat_ids = self.coco.getCatIds()\n","            cats = self.coco.loadCats(cat_ids)\n","\n","            # masks : size가 (height x width)인 2D\n","            # 각각의 pixel 값에는 \"category id + 1\" 할당\n","            # Background = 0\n","            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n","            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n","            for i in range(len(anns)):\n","                className = get_classname(anns[i]['category_id'], cats)\n","                pixel_value = category_names.index(className)\n","                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n","            masks = masks\n","\n","            # transform -> albumentations 라이브러리 활용\n","            if self.transform is not None:\n","                transformed = self.transform(image=images, mask=masks)\n","                images = transformed[\"image\"]\n","                masks = transformed[\"mask\"]\n","            \n","            return images, masks, image_infos\n","        \n","        if self.mode == 'test':\n","            # transform -> albumentations 라이브러리 활용\n","            if self.transform is not None:\n","                transformed = self.transform(image=images)\n","                images = transformed[\"image\"]\n","            \n","            return images, image_infos\n","    \n","    \n","    def __len__(self) -> int:\n","        # 전체 dataset의 size를 return\n","        return len(self.coco.getImgIds())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNlgdADKNEP5"},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iU9URFSJNfM8"},"source":["def get_validation_augmentations():\n","    return A.Compose([\n","        A.Normalize(mean=CFG['mean'], std=CFG['std'], max_pixel_value=255.0, p=1.0),\n","        ToTensorV2()\n","    ],p=1.0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GA3FwugTM55B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619873221393,"user_tz":-540,"elapsed":787,"user":{"displayName":"주영이","photoUrl":"","userId":"06973549239085587415"}},"outputId":"f090aaed-a0f8-434b-f067-e785be2b3eec"},"source":["test_ds = CustomDataLoader(test_path, mode=\"test\", transform=get_validation_augmentations())\n","\n","test_loader = DataLoader(test_ds, \n","                          batch_size=1, \n","                          num_workers=4,\n","                         collate_fn=collate_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.01s)\n","creating index...\n","index created!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wh7Icj_XM57g"},"source":["def test(models, data_loader, device):\n","  \n","    size = 256\n","    transform = A.Compose([A.Resize(256, 256)])\n","    print('Start prediction.')\n","    file_name_list = []\n","    outs = None\n","    preds_array = np.empty((0, size*size), dtype=np.long)\n","    pbar = tqdm(enumerate(data_loader), total=len(data_loader), position=0, leave=True)\n","    with torch.no_grad():\n","      with zipfile.ZipFile(OUT_MASKS, 'w') as mask_out:\n","        for step, (imgs, image_infos) in pbar:\n","            # print(imgs)\n","            # print(imgs)\n","            imgs = torch.stack(imgs)\n","            for model in models:\n","                # outs = model(imgs.to(device).float())\n","                if outs == None:\n","                    outs = model(imgs.to(device).float())\n","                else:\n","                    outs += model(imgs.to(device).float())\n","\n","            # inference (512 x 512)\n","            \n","            flips = [[-1],[-2],[-2,-1]]\n","            for flip in flips:\n","                flip_img = torch.flip(imgs, flip)\n","                for model in models:\n","                    out = model(flip_img.to(device).float())\n","                    out = torch.flip(out, flip)\n","                    outs += out\n","\n","            oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n","            plt.imshow(np.squeeze(oms))\n","            # print(oms.shape)\n","            # break\n","            file_name = image_infos[0]['file_name'].split(\"/\")\n","            file_name[0] += \"_masks\"\n","            file_name[1] = file_name[1][:-4]\n","            file_name = \"/\".join(file_name)\n","\n","            m = cv2.imencode(\".png\", oms.squeeze())[1]\n","            mask_out.writestr(f\"{file_name}.png\", m)\n","            \n","            # resize (256 x 256)\n","            temp_mask = []\n","            for img, mask in zip(np.stack(imgs), oms):\n","                # print(mask.shape)\n","                transformed = transform(image=img, mask=mask)\n","                mask = transformed['mask']\n","                temp_mask.append(mask)\n","\n","            oms = np.array(temp_mask)\n","            # print(oms.shape)\n","            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n","            # print(oms.shape)\n","            # return\n","            preds_array = np.vstack((preds_array, oms))\n","            \n","            file_name_list.append([i['file_name'] for i in image_infos])\n","    print(\"End prediction.\")\n","    file_names = [y for x in file_name_list for y in x]\n","    \n","    return file_names, preds_array"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwIdmVB5n6gZ"},"source":["models = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"saf_RHJGn9_K"},"source":["for path in MODEL_PATHS:\n","    model = smp.DeepLabV3Plus(CFG[\"encoder\"], encoder_weights=None, in_channels=3, classes=12).to(device)\n","    checkpoint = torch.load(path, map_location=device)\n","    # model.load_state_dict(checkpoint['model'])\n","    model.load_state_dict(checkpoint)\n","    model.eval()\n","    model.to(device)\n","    models.append(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V05JTwJxei1z","executionInfo":{"status":"ok","timestamp":1619873676408,"user_tz":-540,"elapsed":309245,"user":{"displayName":"주영이","photoUrl":"","userId":"06973549239085587415"}},"outputId":"35688938-c5d7-4a16-e159-080ff7ec9fab"},"source":["# test set에 대한 prediction\n","file_names, preds = test(models, test_loader, device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Start prediction.\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 837/837 [05:08<00:00,  2.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["End prediction.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ToUJ_2YselPw"},"source":["# PredictionString 대입\n","for file_name, string in zip(file_names, preds):\n","    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n","                                   ignore_index=True)\n","\n","# submission.csv로 저장\n","submission.to_csv(SUBMISSION_PATH, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgpwNAwrPoDo"},"source":[""],"execution_count":null,"outputs":[]}]}