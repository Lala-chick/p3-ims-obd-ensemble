{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"boostcamp_seg_inference.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1pPD4eZAj8FcNnJfEHmyydEibFsiV6TuC","authorship_tag":"ABX9TyMUTAeYzgL+gz/0iwoAvMPC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRIG5nkJkAr8","executionInfo":{"status":"ok","timestamp":1620194371608,"user_tz":-540,"elapsed":20915,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}},"outputId":"5424815e-fb1c-4c06-fba7-d5c8efa7c40f"},"source":["!pip install segmentation_models_pytorch\n","!pip install -q -U albumentations\n","!pip install timm\n","!pip install fastai --upgrade"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting segmentation_models_pytorch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/54/8953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a/segmentation_models_pytorch-0.1.3-py3-none-any.whl (66kB)\n","\u001b[K     |████████████████████████████████| 71kB 6.6MB/s \n","\u001b[?25hCollecting timm==0.3.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/2d/39ecc56fbb202e1891c317e8e44667299bc3b0762ea2ed6aaaa2c2f6613c/timm-0.3.2-py3-none-any.whl (244kB)\n","\u001b[K     |████████████████████████████████| 245kB 19.6MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.9.1+cu101)\n","Collecting efficientnet-pytorch==0.6.3\n","  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n","Collecting pretrainedmodels==0.7.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from timm==0.3.2->segmentation_models_pytorch) (1.8.1+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.19.5)\n","Collecting munch\n","  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.41.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->timm==0.3.2->segmentation_models_pytorch) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp37-none-any.whl size=12420 sha256=17ea27ab60b15a7b3f9f85941f1126d31fc781c5ba957de31f977b0effd413bf\n","  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp37-none-any.whl size=60963 sha256=353561898ec2d1dd6218fa571ce91d1194a248e3d3ed70c054119d5c0bf2c9c6\n","  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: timm, efficientnet-pytorch, munch, pretrainedmodels, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.3 timm-0.3.2\n","\u001b[K     |████████████████████████████████| 81kB 7.1MB/s \n","\u001b[K     |████████████████████████████████| 37.6MB 115kB/s \n","\u001b[K     |████████████████████████████████| 952kB 66.2MB/s \n","\u001b[?25hRequirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.3.2)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->timm) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->timm) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Collecting fastai\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/79/e8a87e4c20238e114671314426227db8647d2b42744eab79e0917c59865e/fastai-2.3.1-py3-none-any.whl (194kB)\n","\u001b[K     |████████████████████████████████| 204kB 13.7MB/s \n","\u001b[?25hCollecting fastcore<1.4,>=1.3.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b0/f1fbf554e0bf3c76e1bdc3b82eedfe41fcf656479586be38c64421082b1b/fastcore-1.3.20-py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n","Requirement already satisfied, skipping upgrade: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n","Requirement already satisfied, skipping upgrade: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.9.1+cu101)\n","Requirement already satisfied, skipping upgrade: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (20.9)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n","Requirement already satisfied, skipping upgrade: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.0)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: torch<1.9,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.8.1+cu101)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n","Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (19.3.1)\n","Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.1.5)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n","Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.19.5)\n","Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.10.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.4.7)\n","Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.4.1)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.5)\n","Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n","Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.1.3)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.8.2)\n","Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.5)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (56.0.0)\n","Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (7.4.0)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.41.1)\n","Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.0)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.0.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9,>=1.7.0->fastai) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->fastai) (1.15.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.10.1)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.4.1)\n","Installing collected packages: fastcore, fastai\n","  Found existing installation: fastai 1.0.61\n","    Uninstalling fastai-1.0.61:\n","      Successfully uninstalled fastai-1.0.61\n","Successfully installed fastai-2.3.1 fastcore-1.3.20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yg-mrKYjkF3T","executionInfo":{"status":"ok","timestamp":1620195910027,"user_tz":-540,"elapsed":3650,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["import segmentation_models_pytorch as smp\n","from tqdm import tqdm\n","import gc\n","\n","import math\n","from torch.optim.optimizer import Optimizer, required\n","\n","from fastai.vision.all import *\n","\n","from sklearn.model_selection import GroupKFold, KFold\n","import torch\n","from torch import nn\n","import torchvision\n","import cv2\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset,DataLoader\n","from torch.utils.data.sampler import SequentialSampler, RandomSampler\n","from torch.cuda.amp import autocast, GradScaler\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts, _LRScheduler\n","from scipy.ndimage.interpolation import zoom\n","import albumentations as A\n","from torch.nn import functional as F\n","from albumentations.pytorch import ToTensorV2\n","\n","from pycocotools.coco import COCO\n","\n","import matplotlib.pyplot as plt\n","import sys\n","import time\n","import random\n","import timm\n","\n","import zipfile"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMHJzp3QkF5r","executionInfo":{"status":"ok","timestamp":1620195910030,"user_tz":-540,"elapsed":3648,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["CFG = {\n","    \"mean\": (0.485, 0.456, 0.406),\n","    \"std\": (0.229, 0.224, 0.225)\n","}"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"uAzRvKRckF8C","executionInfo":{"status":"ok","timestamp":1620195910030,"user_tz":-540,"elapsed":3644,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["device = \"cuda\"\n","\n","test_path = '/content/drive/MyDrive/boostcamp_imgseg/test.json'\n","\n","\n","MODEL_PATHS_unext = ['/content/drive/MyDrive/boostcamp_imgseg/models/unext/resnext50_0.pth',\n","                     '/content/drive/MyDrive/boostcamp_imgseg/models/unext/resnext50_1.pth',\n","                     '/content/drive/MyDrive/boostcamp_imgseg/models/unext/resnext50_2.pth',\n","                     '/content/drive/MyDrive/boostcamp_imgseg/models/unext/resnext50_3.pth',\n","                     '/content/drive/MyDrive/boostcamp_imgseg/models/unext/resnext50_4.pth']\n","\n","MODEL_PATHS_resnext50 = ['/content/drive/MyDrive/boostcamp_imgseg/models/resnext50_32x4d/0_checkpoint.pt',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/resnext50_32x4d/1_checkpoint.pt',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/resnext50_32x4d/2_checkpoint.pt',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/resnext50_32x4d/3_checkpoint.pt',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/resnext50_32x4d/4_checkpoint.pt']    \n","\n","MODEL_PATHS_seresnet101 = ['/content/drive/MyDrive/boostcamp_imgseg/models/se_resnet101/se_resnet101_0fold.pth',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/se_resnet101/se_resnet101_1fold.pth',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/se_resnet101/se_resnet101_2fold.pth',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/se_resnet101/se_resnet101_3fold.pth',\n","                            '/content/drive/MyDrive/boostcamp_imgseg/models/se_resnet101/se_resnet101_4fold.pth',]                                  \n","\n","SUBMISSION_PATH = \"/content/drive/MyDrive/boostcamp_imgseg/submission.csv\"\n","OUT_MASKS = f'/content/drive/MyDrive/boostcamp_imgseg/pseudo_masks_jy.zip'\n","\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UB12SrWkF-g","executionInfo":{"status":"ok","timestamp":1620195910031,"user_tz":-540,"elapsed":3640,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6MhJScZE_ne","executionInfo":{"status":"ok","timestamp":1620195910031,"user_tz":-540,"elapsed":3636,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["class FPN(nn.Module):\n","    def __init__(self, input_channels:list, output_channels:list):\n","        super().__init__()\n","        self.convs = nn.ModuleList(\n","            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n","             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n","             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n","            for in_ch, out_ch in zip(input_channels, output_channels)])\n","        \n","    def forward(self, xs:list, last_layer):\n","        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n","               for i,(c,x) in enumerate(zip(self.convs, xs))]\n","        hcs.append(last_layer)\n","        return torch.cat(hcs, dim=1)\n","\n","class UnetBlock(Module):\n","    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n","                 self_attention:bool=False, **kwargs):\n","        super().__init__()\n","        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n","        self.bn = nn.BatchNorm2d(x_in_c)\n","        ni = up_in_c//2 + x_in_c\n","        nf = nf if nf is not None else max(up_in_c//2,32)\n","        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n","        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n","            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n","        s = left_in\n","        up_out = self.shuf(up_in)\n","        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n","        return self.conv2(self.conv1(cat_x))\n","        \n","class _ASPPModule(nn.Module):\n","    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n","        super().__init__()\n","        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n","                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n","        self.bn = nn.BatchNorm2d(planes)\n","        self.relu = nn.ReLU()\n","\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x = self.atrous_conv(x)\n","        x = self.bn(x)\n","\n","        return self.relu(x)\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","class ASPP(nn.Module):\n","    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n","        super().__init__()\n","        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n","            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n","        self.aspps = nn.ModuleList(self.aspps)\n","        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n","                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n","                        nn.BatchNorm2d(mid_c), nn.ReLU())\n","        out_c = out_c if out_c is not None else mid_c\n","        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n","                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n","        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x0 = self.global_pool(x)\n","        xs = [aspp(x) for aspp in self.aspps]\n","        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat([x0] + xs, dim=1)\n","        return self.out_conv(x)\n","    \n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","class UneXt50(nn.Module):\n","    def __init__(self, stride=1, **kwargs):\n","        super().__init__()\n","        #encoder\n","        # m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models',\n","        #                    'resnext50_32x4d_ssl')\n","        m = timm.create_model(\"swsl_resnext50_32x4d\", pretrained=True)\n","        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n","        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n","                            m.layer1) #256\n","        self.enc2 = m.layer2 #512\n","        self.enc3 = m.layer3 #1024\n","        self.enc4 = m.layer4 #2048\n","        #aspp with customized dilatations\n","        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n","        self.drop_aspp = nn.Dropout2d(0.5)\n","        #decoder\n","        self.dec4 = UnetBlock(512,1024,256)\n","        self.dec3 = UnetBlock(256,512,128)\n","        self.dec2 = UnetBlock(128,256,64)\n","        self.dec1 = UnetBlock(64,64,32)\n","        self.fpn = FPN([512,256,128,64],[16]*4)\n","        self.drop = nn.Dropout2d(0.1)\n","        self.final_conv = ConvLayer(32+16*4, 12, ks=1, norm_type=None, act_cls=None)\n","        \n","    def forward(self, x):\n","        enc0 = self.enc0(x)\n","        enc1 = self.enc1(enc0)\n","        enc2 = self.enc2(enc1)\n","        enc3 = self.enc3(enc2)\n","        enc4 = self.enc4(enc3)\n","        enc5 = self.aspp(enc4)\n","        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n","        dec2 = self.dec3(dec3,enc2)\n","        dec1 = self.dec2(dec2,enc1)\n","        dec0 = self.dec1(dec1,enc0)\n","        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n","        x = self.final_conv(self.drop(x))\n","        x = F.interpolate(x,scale_factor=2,mode='bilinear')\n","        return x"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"mt_trEwjkGA-","executionInfo":{"status":"ok","timestamp":1620195910031,"user_tz":-540,"elapsed":3630,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["class CustomDataLoader(Dataset):\n","    \"\"\"COCO format\"\"\"\n","    def __init__(self, data_dir, mode = 'train', transform = None):\n","        super().__init__()\n","        self.mode = mode\n","        self.transform = transform\n","        self.coco = COCO(data_dir)\n","        \n","    def __getitem__(self, index: int):\n","        # dataset이 index되어 list처럼 동작\n","        image_id = self.coco.getImgIds(imgIds=index)\n","        image_infos = self.coco.loadImgs(image_id)[0]\n","        \n","        # cv2 를 활용하여 image 불러오기\n","        images = cv2.imread(os.path.join('/content/drive/MyDrive/boostcamp_imgseg', image_infos['file_name']))\n","        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n","        \n","        if (self.mode in ('train', 'val')):\n","            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n","            anns = self.coco.loadAnns(ann_ids)\n","\n","            # Load the categories in a variable\n","            cat_ids = self.coco.getCatIds()\n","            cats = self.coco.loadCats(cat_ids)\n","\n","            # masks : size가 (height x width)인 2D\n","            # 각각의 pixel 값에는 \"category id + 1\" 할당\n","            # Background = 0\n","            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n","            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n","            for i in range(len(anns)):\n","                className = get_classname(anns[i]['category_id'], cats)\n","                pixel_value = category_names.index(className)\n","                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n","            masks = masks\n","\n","            # transform -> albumentations 라이브러리 활용\n","            if self.transform is not None:\n","                transformed = self.transform(image=images, mask=masks)\n","                images = transformed[\"image\"]\n","                masks = transformed[\"mask\"]\n","            \n","            return images, masks, image_infos\n","        \n","        if self.mode == 'test':\n","            # transform -> albumentations 라이브러리 활용\n","            if self.transform is not None:\n","                transformed = self.transform(image=images)\n","                images = transformed[\"image\"]\n","            \n","            return images, image_infos\n","    \n","    \n","    def __len__(self) -> int:\n","        # 전체 dataset의 size를 return\n","        return len(self.coco.getImgIds())"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"FaEfguYGkGDK","executionInfo":{"status":"ok","timestamp":1620195910032,"user_tz":-540,"elapsed":3627,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"4K7E2bGJkGFj","executionInfo":{"status":"ok","timestamp":1620195910032,"user_tz":-540,"elapsed":3623,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["def get_validation_augmentations():\n","    return A.Compose([\n","        A.Normalize(mean=CFG['mean'], std=CFG['std'], max_pixel_value=255.0, p=1.0),\n","        ToTensorV2()\n","    ],p=1.0)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFah3BNXkGH8","executionInfo":{"status":"ok","timestamp":1620195910032,"user_tz":-540,"elapsed":3618,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}},"outputId":"40bc360d-b21a-4601-f519-6f850043d979"},"source":["test_ds = CustomDataLoader(test_path, mode=\"test\", transform=get_validation_augmentations())\n","\n","test_loader = DataLoader(test_ds, \n","                          batch_size=1, \n","                          num_workers=4,\n","                         shuffle=False,\n","                         collate_fn=collate_fn)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.01s)\n","creating index...\n","index created!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YCamZiMJkGKa","executionInfo":{"status":"ok","timestamp":1620196933390,"user_tz":-540,"elapsed":716,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["def inference(models, imgs, device):\n","    outs = None\n","    flip_outs = None\n","    flips = [[-1],[-2],[-2,-1]]\n","    # flips = [[-1]]\n","    for model in models:\n","        model.eval()\n","        if outs == None:\n","            outs = F.softmax(model(imgs.to(device).float()), dim=1).detach()\n","        else:\n","            outs += F.softmax(model(imgs.to(device).float()), dim=1).detach()\n","        \n","    # outs /= len(models)\n","        \n","    for flip in flips:\n","        flip_img = torch.flip(imgs, flip)\n","        tmp_outs = None\n","        for model in models:\n","            flip_out = F.softmax(model(flip_img.to(device).float()), dim=1).detach()\n","            flip_out = torch.flip(flip_out, flip)\n","            if tmp_outs == None:\n","                tmp_outs = flip_out\n","            else:\n","                tmp_outs += flip_out\n","        tmp_outs /= len(models)\n","        if flip_outs == None:\n","            flip_outs = tmp_outs\n","        else:\n","            flip_outs += tmp_outs\n","    # flip_outs /= len(flips)\n","    \n","    outs += flip_outs\n","    \n","    return outs/(len(models) + len(flips))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"eVKCxH9FkGM8","executionInfo":{"status":"ok","timestamp":1620195910456,"user_tz":-540,"elapsed":1519,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["def test(models1, models2, models3, data_loader, device):\n","  \n","    size = 256\n","    transform = A.Compose([A.Resize(256, 256)])\n","    print('Start prediction.')\n","    file_name_list = []\n","    preds_array = np.empty((0, size*size), dtype=np.long)\n","    pbar = tqdm(enumerate(data_loader), total=len(data_loader), position=0, leave=True)\n","    with torch.no_grad():\n","        with zipfile.ZipFile(OUT_MASKS, 'w') as mask_out:\n","            for step, (imgs, image_infos) in pbar:\n","                # print(imgs)\n","                # print(imgs)\n","                imgs = torch.stack(imgs)\n","                \n","                outs1 = inference(models1, imgs, device)\n","                outs2 = inference(models2, imgs, device)\n","                outs3 = inference(models3, imgs, device)\n","\n","                outs = (outs1 * 0.4) + (outs2 * 0.3) + (outs3 * 0.3)\n","\n","                oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n"," \n","                file_name = image_infos[0]['file_name'].split(\"/\")\n","                file_name[0] += \"_masks\"\n","                file_name[1] = file_name[1][:-4]\n","                file_name = \"/\".join(file_name)\n","\n","                m = cv2.imencode(\".png\", oms.squeeze())[1]\n","                mask_out.writestr(f\"{file_name}.png\", m)\n","\n","                # resize (256 x 256)\n","                temp_mask = []\n","                for img, mask in zip(np.stack(imgs), oms):\n","                    # print(mask.shape)\n","                    transformed = transform(image=img, mask=mask)\n","                    mask = transformed['mask']\n","                    temp_mask.append(mask)\n","\n","                oms = np.array(temp_mask)\n","                oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n","\n","                preds_array = np.vstack((preds_array, oms))\n","\n","                file_name_list.append([i['file_name'] for i in image_infos])\n","    print(\"End prediction.\")\n","    file_names = [y for x in file_name_list for y in x]\n","    \n","    return file_names, preds_array"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9M3dvmLkGPW","executionInfo":{"status":"ok","timestamp":1620195910456,"user_tz":-540,"elapsed":1279,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["models_resnext50 = []\n","models_unext = []\n","models_seresnet101 = []\n","# models_resnext101 = []"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8H6KdtRbHVR2"},"source":["## ResNext50"]},{"cell_type":"code","metadata":{"id":"3F1ERV2HkMwo","executionInfo":{"status":"ok","timestamp":1620195918456,"user_tz":-540,"elapsed":8132,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["for path in MODEL_PATHS_resnext50:\n","    model = smp.DeepLabV3Plus(\"resnext50_32x4d\", encoder_weights=None, in_channels=3, classes=12).to(device)\n","    checkpoint = torch.load(path)\n","    # model.load_state_dict(checkpoint['model'])\n","    model.load_state_dict(checkpoint)\n","    model.eval()\n","    model.to(device)\n","    models_resnext50.append(model)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZEO4uc4QHaBf"},"source":["## UneXt"]},{"cell_type":"code","metadata":{"id":"UMp0I6qRR1su","executionInfo":{"status":"ok","timestamp":1620195926488,"user_tz":-540,"elapsed":15231,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["for path in MODEL_PATHS_unext:\n","    model = UneXt50().to(device)\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model'])\n","    # model.load_state_dict(checkpoint)\n","    model.eval()\n","    model.to(device)\n","    models_unext.append(model)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CK_-0R9aHibi"},"source":["## ResNet101"]},{"cell_type":"code","metadata":{"id":"EJZ4V4pOHkIj","executionInfo":{"status":"ok","timestamp":1620195935442,"user_tz":-540,"elapsed":22882,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["for path in MODEL_PATHS_seresnet101:\n","    model = smp.DeepLabV3Plus(\"se_resnet101\", encoder_weights=None, in_channels=3, classes=12).to(device)\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model'])\n","    # model.load_state_dict(checkpoint)\n","    model.eval()\n","    model.to(device)\n","    models_seresnet101.append(model)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGiQ593CkMy6","executionInfo":{"status":"ok","timestamp":1620198340919,"user_tz":-540,"elapsed":1402632,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}},"outputId":"a180c71d-c661-4be9-b85c-fa814af8728f"},"source":["# test set에 대한 prediction\n","file_names, preds = test(models_resnext50, models_unext, models_seresnet101, test_loader, device)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Start prediction.\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/837 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n","  \"See the documentation of nn.Upsample for details.\".format(mode)\n","100%|██████████| 837/837 [23:21<00:00,  1.67s/it]"],"name":"stderr"},{"output_type":"stream","text":["End prediction.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bvcXGIuYkM1P","executionInfo":{"status":"ok","timestamp":1620198408012,"user_tz":-540,"elapsed":19523,"user":{"displayName":"Jooyoung Lee","photoUrl":"","userId":"15910579598830381536"}}},"source":["# PredictionString 대입\n","\n","submission = pd.read_csv('/content/drive/MyDrive/boostcamp_imgseg/sample_submission.csv', index_col=None)\n","\n","for file_name, string in zip(file_names, preds):\n","    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n","                                   ignore_index=True)\n","\n","# submission.csv로 저장\n","submission.to_csv(SUBMISSION_PATH, index=False)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"oAPJh12wkM-8"},"source":[""],"execution_count":null,"outputs":[]}]}