{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9065c9-fba2-43be-9c0a-1df8d5aba36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/opt/ml/myfolder/segmentation_models.pytorch-master')\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import math\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts, _LRScheduler\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "import albumentations as A\n",
    "from torch.nn import functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import timm\n",
    "\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbc024e-a099-4e33-994b-d237b8718e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"mean\": (0.485, 0.456, 0.406),\n",
    "    \"std\": (0.229, 0.224, 0.225)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ad81de-4f25-40c0-ac94-1b668f6ac45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "test_path = '/opt/ml/input/data/test.json'\n",
    "\n",
    "MODEL_PATHS_serenext101 = ['/opt/ml/myfolder/models/DeepLabV3Plus/se_resnext101_32x4d/se_resnext101_32x4d_0.pth',\n",
    "                          '/opt/ml/myfolder/models/DeepLabV3Plus/se_resnext101_32x4d/se_resnext101_32x4d_1.pth',\n",
    "                          '/opt/ml/myfolder/models/DeepLabV3Plus/se_resnext101_32x4d/se_resnext101_32x4d_2.pth',\n",
    "                          '/opt/ml/myfolder/models/DeepLabV3Plus/se_resnext101_32x4d/se_resnext101_32x4d_3.pth',\n",
    "                          '/opt/ml/myfolder/models/DeepLabV3Plus/se_resnext101_32x4d/se_resnext101_32x4d_4.pth']\n",
    "\n",
    "MODEL_PATHS_renext101 = ['/content/drive/MyDrive/trash_segmentation/models/resnext50_32x4d_0.pth']\n",
    "\n",
    "SUBMISSION_PATH = \"/opt/ml/code/submission/submission.csv\"\n",
    "OUT_MASKS = f'/opt/ml/myfolder/masks.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbdea2f-8683-4ffa-b476-2f33340d26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf30037-136d-49dc-a0ca-f06e78a44bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join('/opt/ml/input/data', image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0fff7e-481c-40c4-aa62-23ce8e89c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ad922e-cda7-4876-ac60-c4edfb10778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_augmentations():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=CFG['mean'], std=CFG['std'], max_pixel_value=255.0, p=1.0),\n",
    "        ToTensorV2()\n",
    "    ],p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106166ab-7a45-4760-9e05-86d1817d9bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "test_ds = CustomDataLoader(test_path, mode=\"test\", transform=get_validation_augmentations())\n",
    "\n",
    "test_loader = DataLoader(test_ds, \n",
    "                          batch_size=1, \n",
    "                          num_workers=4,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91c76686-6710-4752-ae1e-1f9b0dc236ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(models, imgs, device):\n",
    "    outs = None\n",
    "    flip_outs = None\n",
    "    flips = [[-1],[-2],[-2,-1]]\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        if outs == None:\n",
    "            outs = model(imgs.to(device).float()).detach()\n",
    "        else:\n",
    "            outs += model(imgs.to(device).float()).detach()\n",
    "        \n",
    "    outs /= len(models)\n",
    "        \n",
    "    for flip in flips:\n",
    "        flip_img = torch.flip(imgs, flip)\n",
    "        tmp_outs = None\n",
    "        for model in models:\n",
    "            flip_out = model(flip_img.to(device).float()).detach()\n",
    "            flip_out = torch.flip(flip_out, flip)\n",
    "            if tmp_outs == None:\n",
    "                tmp_outs = flip_out\n",
    "            else:\n",
    "                tmp_outs += flip_out\n",
    "        tmp_outs /= len(models)\n",
    "        if flip_outs == None:\n",
    "            flip_outs = tmp_outs\n",
    "        else:\n",
    "            flip_outs += tmp_outs\n",
    "    flip_outs /= 3\n",
    "    \n",
    "    outs += flip_outs\n",
    "    \n",
    "    return outs/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16fcf40e-2b1d-4d97-a16a-c9ef44976060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, data_loader, device):\n",
    "  \n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader), position=0, leave=True)\n",
    "    with torch.no_grad():\n",
    "        with zipfile.ZipFile(OUT_MASKS, 'w') as mask_out:\n",
    "            for step, (imgs, image_infos) in pbar:\n",
    "                # print(imgs)\n",
    "                # print(imgs)\n",
    "                imgs = torch.stack(imgs)\n",
    "                \n",
    "                outs = inference(models, imgs, device)\n",
    "\n",
    "                oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    " \n",
    "                file_name = image_infos[0]['file_name'].split(\"/\")\n",
    "                file_name[0] += \"_masks\"\n",
    "                file_name[1] = file_name[1][:-4]\n",
    "                file_name = \"/\".join(file_name)\n",
    "\n",
    "                m = cv2.imencode(\".png\", oms.squeeze())[1]\n",
    "                mask_out.writestr(f\"{file_name}.png\", m)\n",
    "\n",
    "                # resize (256 x 256)\n",
    "                temp_mask = []\n",
    "                for img, mask in zip(np.stack(imgs), oms):\n",
    "                    # print(mask.shape)\n",
    "                    transformed = transform(image=img, mask=mask)\n",
    "                    mask = transformed['mask']\n",
    "                    temp_mask.append(mask)\n",
    "\n",
    "                oms = np.array(temp_mask)\n",
    "                oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "\n",
    "                preds_array = np.vstack((preds_array, oms))\n",
    "\n",
    "                file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96ea85cd-05cb-4cc3-b4ae-6995803ef4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.DeepLabV3Plus(\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=12).to(device)\n",
    "# checkpoint = torch.load('/opt/ml/myfolder/models/DeepLabV3Plus/se_resnext101_32x4d/se_resnext101_32x4d_0.pth', map_location=device)\n",
    "# model.load_state_dict(checkpoint['model'])\n",
    "# # model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51266bd1-ea9c-4d16-915e-144bda855279",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236f0f4-e700-47e3-be82-5fed2117d21f",
   "metadata": {},
   "source": [
    "## SE-ResNeXt 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be7be5fa-9364-44ca-9ff0-6baeffe5662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in MODEL_PATHS_serenext101:\n",
    "    model = smp.DeepLabV3Plus(\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=12)\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    # model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    models.append(model)\n",
    "    \n",
    "    del checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf5760-3743-46da-a7b1-3079db77928d",
   "metadata": {},
   "source": [
    "## ResNeXt 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd4b2fe-2be9-44f3-9409-26389188702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in MODEL_PATHS_renext101:\n",
    "#     model = smp.DeepLabV3Plus(\"resnext101_32x8d\", encoder_weights=None, in_channels=3, classes=12).to(device)\n",
    "#     checkpoint = torch.load(path, map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model'])\n",
    "#     # model.load_state_dict(checkpoint)\n",
    "#     model.eval()\n",
    "#     model.to(device)\n",
    "#     models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d2dc976-329a-44ad-8d52-dcff20fd8a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 837/837 [33:38<00:00,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End prediction.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test set에 대한 prediction\n",
    "file_names, preds = test(models, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c08c0e-9e81-4a12-9789-ee28ab9f4984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PredictionString 대입\n",
    "\n",
    "submission = pd.read_csv('/opt/ml/code/submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb546a-c03c-4f1a-81b7-09e9d9333183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af42be2-c62e-4dbd-b077-b0ea1423ee68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aeef56-fe71-45f4-aed2-713a28b4832e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120618a2-42fc-4bc4-aa45-a495c88b558f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
