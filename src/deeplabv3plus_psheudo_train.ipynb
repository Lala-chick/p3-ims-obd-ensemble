{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:58.944902Z",
     "start_time": "2021-04-22T11:06:56.623974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.5.0+cu101\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla V100-PCIE-32GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import *\n",
    "import cv2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import zipfile\n",
    "\n",
    "# Pretrained Model\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# torchvision Models\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 세팅 및 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.171980Z",
     "start_time": "2021-04-22T11:06:59.167952Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 20  # Mini-batch size\n",
    "num_epochs = 40\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:06:59.446510Z",
     "start_time": "2021-04-22T11:06:59.443508Z"
    }
   },
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashDataset(Dataset):\n",
    "    def __init__(self, df, root='../input/data/', mode=\"train\", transform=None):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "        self.mask_label=[]\n",
    "        mask_path = self.root + self.df['masks']\n",
    "        for m_path in tqdm(mask_path.tolist()):\n",
    "            masks = cv2.imread(m_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "            self.fold_label(masks)\n",
    "    \n",
    "    def fold_label(self, masks):\n",
    "        len(list(np.unique(masks)))\n",
    "        if 10.0 in list(np.unique(masks)): self.mask_label.append(0)\n",
    "        elif 1.0 in list(np.unique(masks)): self.mask_label.append(1)\n",
    "        elif 11.0 in list(np.unique(masks)): self.mask_label.append(2)\n",
    "        elif 6.0 in list(np.unique(masks)): self.mask_label.append(3)\n",
    "        elif 5.0 in list(np.unique(masks)): self.mask_label.append(4)\n",
    "        elif 4.0 in list(np.unique(masks)): self.mask_label.append(5)\n",
    "        elif 8.0 in list(np.unique(masks)): self.mask_label.append(6)\n",
    "        elif 7.0 in list(np.unique(masks)): self.mask_label.append(7)\n",
    "        elif 2.0 in list(np.unique(masks)): self.mask_label.append(8)\n",
    "        elif 3.0 in list(np.unique(masks)): self.mask_label.append(9)\n",
    "        elif 9.0 in list(np.unique(masks)): self.mask_label.append(10)\n",
    "        else: self.mask_label.append(11)  \n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.root + self.df.iloc[idx]['filepath']\n",
    "        imgs = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.mode==\"train\" or self.mode==\"val\":\n",
    "            mask_path = self.root + self.df.iloc[idx]['masks']\n",
    "            masks = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
    "            transformed = self.transform(image=imgs, mask=masks)\n",
    "            imgs = transformed[\"image\"]\n",
    "            masks = transformed[\"mask\"]\n",
    "            return imgs, masks\n",
    "        \n",
    "        elif self.mode == \"test\":\n",
    "            transformed = self.transform(image=imgs)\n",
    "            imgs = transformed[\"image\"]\n",
    "            return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            A.Rotate(border_mode=1, p=0.5),\n",
    "                            A.ShiftScaleRotate(border_mode=1, p=0.5),\n",
    "                            A.HorizontalFlip(p=0.5),\n",
    "                            A.VerticalFlip(p=0.5),\n",
    "                            A.Cutout(),\n",
    "                            A.Normalize(\n",
    "                                mean=(0.485, 0.456, 0.406),\n",
    "                                std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "                            ),\n",
    "                            ToTensorV2(),\n",
    "                            ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Normalize(\n",
    "                                mean=(0.485, 0.456, 0.406),\n",
    "                                std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0\n",
    "                            ),\n",
    "                            ToTensorV2(),\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/data/train.csv\")\n",
    "\n",
    "additional_df = pd.read_csv(\"../input/data/test.csv\")\n",
    "df = pd.concat([df, additional_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099a397d37cb42f292a3eb29ace5508e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = TrashDataset(df, mode=\"train\", transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_score(avrg_class_IoU):\n",
    "    # Class Score\n",
    "    class_name=['BG','UNK','General Trash','Paper','Paper pack','Metal','Glass','Plastic','Styrofoam','Plastic Bag','Battery','Clothing']\n",
    "    print('-'*80)\n",
    "    print('Validation Class Pred mIoU Score')\n",
    "    for idx, class_score in enumerate(avrg_class_IoU):\n",
    "        print('[{}] mIoU : [{:.4f}]'.format(class_name[idx],class_score))\n",
    "    print('-'*80) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(W, H, lam):\n",
    "    cut_rat = torch.sqrt(1.0 - lam)\n",
    "    cut_w = (W * cut_rat).type(torch.long)\n",
    "    cut_h = (H * cut_rat).type(torch.long)\n",
    "    # uniform\n",
    "    cx = torch.randint(W, (1,)).to(device)\n",
    "    cy = torch.randint(H, (1,)).to(device)\n",
    "    x1 = torch.clamp(cx - cut_w // 2, 0, W)\n",
    "    y1 = torch.clamp(cy - cut_h // 2, 0, H)\n",
    "    x2 = torch.clamp(cx + cut_w // 2, 0, W)\n",
    "    y2 = torch.clamp(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_data(x, y, alpha=1.0, p=0.3):\n",
    "    # x-> img \n",
    "    # y-> mask\n",
    "    if np.random.random() > p:\n",
    "        return x, y\n",
    "    W, H = x.size(2), x.size(3)\n",
    "    shuffle = torch.randperm(x.size(0)).to(device)\n",
    "    cutmix_x = x\n",
    "    cutmix_y = y\n",
    "\n",
    "    lam = torch.distributions.beta.Beta(alpha, alpha).sample().to(device)\n",
    "    x1, y1, x2, y2 = rand_bbox(W, H, lam)\n",
    "    cutmix_x[:, :, x1:x2, y1:y2] = x[shuffle, :, x1:x2, y1:y2]\n",
    "    cutmix_y[:, x1:x2, y1:y2] = y[shuffle, x1:x2, y1:y2]\n",
    "    # Adjust lambda to match pixel ratio\n",
    "    #lam = 1 - ((x2 - x1) * (y2 - y1) / float(W * H)).item()\n",
    "    return cutmix_x, cutmix_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, validation, test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.201874Z",
     "start_time": "2021-04-22T11:15:38.187884Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(fold, num_epochs, model, data_loader, val_loader, criterion, optimizer, scheduler, device):\n",
    "    print('-'*80)\n",
    "    print(f'Fold : [{fold}] Start training..')\n",
    "    print('-'*80)\n",
    "    early_stop=EarlyStopping(patience=5,path='./saved/'+str(fold)+'_checkpoint.pt')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss=[]\n",
    "        model.train()\n",
    "        for step, (images, masks) in enumerate(tqdm(data_loader)):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "\n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            #Cut-Mix\n",
    "            images, masks = cutmix_data(images, masks)\n",
    "\n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "\n",
    "            # loss 계산\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            #Cosine Annealing Scheduler\n",
    "            #scheduler.step(epoch + step / len(data_loader))\n",
    "\n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        avrg_loss, avrg_IoU, avrg_class_IoU, pb_IoU = validation(model, val_loader, device)\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f} Vali Loss: {:.4f}, Vali mIoU: {:.4f}, Vali Public mIoU: {:.4f}'.format(epoch+1, num_epochs, np.mean(train_loss), avrg_loss, avrg_IoU, pb_IoU))\n",
    "        # Reduce Scheduler\n",
    "        scheduler.step(avrg_IoU)\n",
    "        # Class Score\n",
    "        #class_score(avrg_class_IoU)\n",
    "        # Save\n",
    "        early_stop(avrg_IoU,model)\n",
    "        if early_stop.early_stop:\n",
    "            print('Stop Training.....')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T11:15:38.901226Z",
     "start_time": "2021-04-22T11:15:38.888195Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, data_loader, device):\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    n_class=12\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    print('Start validation')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        pb_mIoU_list = []\n",
    "        for step, (images, masks) in enumerate(tqdm(data_loader)):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)        \n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            hist = add_hist(hist, masks.detach().cpu().numpy(), outputs, n_class=n_class)\n",
    "            pb_mIoU = public_label_accuracy_score(masks.detach().cpu().numpy(), outputs, n_class=12)\n",
    "            pb_mIoU_list.append(pb_mIoU)\n",
    "        \n",
    "        vali_mIoU, vali_class_mIoU = label_accuracy_score(hist)\n",
    "        avrg_loss = total_loss / cnt\n",
    "\n",
    "    return avrg_loss, vali_mIoU, vali_class_mIoU, np.mean(pb_mIoU_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 및 Loss function, Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-22T11:15:43.700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Fold : [0] Start training..\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0009320faaa24950845c3032f4966462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfbfbf83b0048119550ac184122fd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Train Loss: 0.9458 Vali Loss: 0.4891, Vali mIoU: 0.3748, Vali Public mIoU: 0.4899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f417542a6b5444bea056f928d12e7b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fe43dbc75d45f68dd5f5f4847ed909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40], Train Loss: 0.4389 Vali Loss: 0.3798, Vali mIoU: 0.4331, Vali Public mIoU: 0.5254\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9784f10e2b444803bf4d4327e22e7bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6dd963dd1cd491ea4301370e10dea76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40], Train Loss: 0.3381 Vali Loss: 0.3165, Vali mIoU: 0.4535, Vali Public mIoU: 0.5669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225c34d4f59642e8a05d4958c0fe46df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0068dd070d463f806332e80b2e4f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40], Train Loss: 0.2955 Vali Loss: 0.3162, Vali mIoU: 0.4567, Vali Public mIoU: 0.5569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0ae6d34cc445a8b294d1cf966c70d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "for current_fold,(train_idx, vali_idx) in enumerate(folds.split(train_dataset,train_dataset.mask_label)):\n",
    "    model = smp.DeepLabV3Plus('resnext50_32x4d', encoder_weights=\"swsl\", classes=12)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    train_data=torch.utils.data.Subset(train_dataset,train_idx)\n",
    "    vali_data=torch.utils.data.Subset(train_dataset,vali_idx)\n",
    "\n",
    "    train_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True,collate_fn=collate_fn,num_workers=0)\n",
    "    val_loader=DataLoader(vali_data,batch_size=1,shuffle=False,collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "    #scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=20, eta_max=learning_rate,  T_up=2, gamma=0.5)\n",
    "    \n",
    "    train(current_fold, num_epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, device)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "297.278px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
