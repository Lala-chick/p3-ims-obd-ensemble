{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sYO7ZAqs4EZ1"
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# faster rcnn model이 포함된 library\n",
    "import torchvision\n",
    "from ensemble_boxes import *\n",
    "from itertools import product\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_paths = ['./saved/0_checkpoint.pth',\n",
    "                      './saved/1_checkpoint.pth',\n",
    "                      './saved/2_checkpoint.pth',\n",
    "                      './saved/3_checkpoint.pth',\n",
    "                      './saved/4_checkpoint.pth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nOkUvLfx4EZ8"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "      data_dir: data가 존재하는 폴더 경로\n",
    "      transforms: data transform (resize, crop, Totensor, etc,,,)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, annotation, data_dir, transforms=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.predictions = {\n",
    "            \"images\": self.coco.dataset[\"images\"].copy(),\n",
    "            \"categories\": self.coco.dataset[\"categories\"].copy(),\n",
    "            \"annotations\": None\n",
    "        }\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxex (x_min, y_min, x_max, y_max)\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        \n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        areas = np.array([x['area'] for x in anns])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "                                \n",
    "        is_crowds = np.array([x['iscrowd'] for x in anns])\n",
    "        is_crowds = torch.as_tensor(is_crowds, dtype=torch.int64)\n",
    "                                \n",
    "        segmentation = np.array([x['segmentation'] for x in anns], dtype=object)\n",
    "\n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([index]), 'area': areas,\n",
    "                  'iscrowd': is_crowds}\n",
    "\n",
    "        # transform\n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': target['boxes'],\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        return image, target, image_id\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a3e2oQP_4EZ9"
   },
   "outputs": [],
   "source": [
    "def get_test_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "annotation = '../input/data/test.json'\n",
    "data_dir = '../input/data'\n",
    "test_dataset = CustomDataset(annotation, data_dir, get_test_transform())\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLabelSmooth(nn.Module):\n",
    "    def __init__(self, num_classes=12, epsilon=0.1, use_gpu=True):\n",
    "        super(CrossEntropyLabelSmooth, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.use_gpu = use_gpu\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        log_probs = self.logsoftmax(inputs)\n",
    "        targets = torch.zeros(log_probs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1)\n",
    "        if self.use_gpu: targets = targets.cuda()\n",
    "        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n",
    "        loss = (- targets * log_probs).mean(0).sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    regression_targets = torch.cat(regression_targets, dim=0)\n",
    "    labal_smooth_loss = CrossEntropyLabelSmooth(num_classes=12)\n",
    "    classification_loss = labal_smooth_loss(class_logits, labels)\n",
    "\n",
    "    sampled_pos_inds_subset = torch.nonzero(labels > 0).squeeze(1)\n",
    "    labels_pos = labels[sampled_pos_inds_subset]\n",
    "    N, num_classes = class_logits.shape\n",
    "    box_regression = box_regression.reshape(N, -1, 4)\n",
    "\n",
    "    box_loss = det_utils.smooth_l1_loss(\n",
    "        box_regression[sampled_pos_inds_subset, labels_pos],\n",
    "        regression_targets[sampled_pos_inds_subset],\n",
    "        beta=1 / 9,\n",
    "        size_average=False,\n",
    "    )\n",
    "    box_loss = box_loss / labels.numel()\n",
    "\n",
    "    return classification_loss, box_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.fastrcnn_loss = fastrcnn_loss\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    model = get_model(12)\n",
    "\n",
    "    # Load the trained weights\n",
    "    checkpoint=torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    del checkpoint\n",
    "\n",
    "    model.eval()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for checkpoint_path in model_weight_paths:\n",
    "    model=load_model(checkpoint_path)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ensemble_predictions(images):\n",
    "    images = list(image.to(device) for image in images)    \n",
    "    result = []\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            result.append(outputs)\n",
    "            del model\n",
    "    return result\n",
    "\n",
    "def run_wbf_ensemble(predictions, image_index, image_size=512, iou_thr=0.5, skip_box_thr=0.005, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist() for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n",
    "    labels = [prediction[image_index]['labels'].tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWheatTTA:\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    image_size = 512\n",
    "\n",
    "    def augment(self, image):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TTAHorizontalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def augment(self, image):\n",
    "        return image.flip(1)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(2)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n",
    "        return boxes\n",
    "\n",
    "class TTAVerticalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return image.flip(2)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(3)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n",
    "        return boxes\n",
    "    \n",
    "class TTARotate90(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return torch.rot90(image, 1, (1, 2))\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        return torch.rot90(images, 1, (2, 3))\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        res_boxes = boxes.copy()\n",
    "        res_boxes[:, [0,2]] = self.image_size - boxes[:, [3,1]] \n",
    "        res_boxes[:, [1,3]] = boxes[:, [0,2]]\n",
    "        return res_boxes\n",
    "\n",
    "class TTACompose(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def augment(self, image):\n",
    "        for transform in self.transforms:\n",
    "            image = transform.augment(image)\n",
    "        return image\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        for transform in self.transforms:\n",
    "            images = transform.batch_augment(images)\n",
    "        return images\n",
    "    \n",
    "    def prepare_boxes(self, boxes):\n",
    "        result_boxes = boxes.copy()\n",
    "        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n",
    "        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n",
    "        return result_boxes\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        for transform in self.transforms[::-1]:\n",
    "            boxes = transform.deaugment_boxes(boxes)\n",
    "        return self.prepare_boxes(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_transforms = []\n",
    "for tta_combination in product([TTAHorizontalFlip(), None], \n",
    "                               [TTAVerticalFlip(), None],\n",
    "                               [TTARotate90(), None]):\n",
    "    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tta_predictions(images, score_threshold=0):\n",
    "    with torch.no_grad():\n",
    "        images = torch.stack(images).float().to(device)\n",
    "        predictions = []\n",
    "        for tta_transform in tta_transforms:\n",
    "            result = []\n",
    "            #ensemble predict\n",
    "            outputs = make_ensemble_predictions(tta_transform.batch_augment(images.clone()))\n",
    "\n",
    "            for i, image in enumerate(images):\n",
    "                boxes, scores, labels = run_wbf_ensemble(outputs, image_index=i)\n",
    "                \n",
    "                indexes = np.where(scores > score_threshold)[0]\n",
    "                boxes = boxes[indexes]\n",
    "                boxes = tta_transform.deaugment_boxes(boxes.copy())\n",
    "                labels = labels[indexes]\n",
    "                result.append({\n",
    "                    'boxes': boxes,\n",
    "                    'scores': scores[indexes],\n",
    "                    'labels': labels[indexes]\n",
    "                })\n",
    "            predictions.append(result)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wbf(predictions, image_index, image_size=512, iou_thr=0.5, skip_box_thr=0.005, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist() for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n",
    "    labels = [prediction[image_index]['labels'].tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/837 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 837/837 [23:40<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    PredictionString              image_id\n",
      "0  8 0.99915993 172 234 464 470 2 0.33760366 160 ...  batch_01_vt/0021.jpg\n",
      "1  8 0.9438883662223816 283 209 490 434 8 0.93482...  batch_01_vt/0028.jpg\n",
      "2  1 0.7964181 72 166 373 382 6 0.15851355 73 161...  batch_01_vt/0031.jpg\n",
      "3  8 0.9354536 122 137 377 369 8 0.608295 32 212 ...  batch_01_vt/0032.jpg\n",
      "4  8 0.9955565333366394 149 1 355 221 8 0.9946048...  batch_01_vt/0070.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(annotation)\n",
    "prediction_strings = []\n",
    "file_names = []\n",
    "results = []\n",
    "for images, _, image_ids in tqdm(test_data_loader):\n",
    "    predictions = make_tta_predictions(images)\n",
    "    for i, image in enumerate(images):\n",
    "        boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "        #boxes = boxes.round().astype(np.int32).clip(min=0, max=511)\n",
    "        image_id = image_ids[i]\n",
    "        image_info = coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        prediction_string = ''\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            prediction_string += str(int(label)-1) + ' ' + str(score) + ' ' + str(box[0]) + ' ' + str(\n",
    "                box[1]) + ' ' + str(box[2]) + ' ' + str(box[3]) + ' '\n",
    "        prediction_strings.append(prediction_string)\n",
    "        file_names.append(image_info['file_name'])\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['PredictionString'] = prediction_strings\n",
    "submission['image_id'] = file_names\n",
    "submission.to_csv(f'FasterRCNN_Kfold_Ensemble.csv', index=None)\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "faster_rcnn_inference.ipynb의 사본",
   "provenance": [
    {
     "file_id": "1Qi6YaWsK-PGLLVVANEJA5xeIpIZTNFci",
     "timestamp": 1620609880752
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
